import asyncio
import requests
from bs4 import BeautifulSoup
from datetime import datetime
import os
import time
import random
from urllib.parse import urljoin
import re
from telegram import Bot
from telegram.constants import ParseMode

class CryptoFollowTracker:
    def __init__(self):
        # ‚úÖ –¢–û–ö–ï–ù–´ –ë–ï–†–£–¢–°–Ø –ò–ó –ü–ï–†–ï–ú–ï–ù–ù–´–• –û–ö–†–£–ñ–ï–ù–ò–Ø (–°–ï–ö–†–ï–¢–û–í)
        self.bot_token = os.environ.get('TELEGRAM_BOT_TOKEN')
        self.channel_id = os.environ.get('TELEGRAM_CHANNEL_ID', '@cryptohobbys')
        
        if not self.bot_token:
            raise ValueError("TELEGRAM_BOT_TOKEN not found in environment variables!")
        
        self.bot = Bot(token=self.bot_token)
        self.base_url = "https://www.rootdata.com"
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Connection': 'keep-alive',
        })
    
    def scrape_xdata_pages(self):
        """–°–∫—Ä–∞–ø–∏–Ω–≥ —Å—Ç—Ä–∞–Ω–∏—Ü X Data –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Å–≤–µ–∂–∏—Ö –¥–∞–Ω–Ω—ã—Ö"""
        all_subscriptions = []
        all_unsubscriptions = []
        
        for page in range(1, 4):
            try:
                print(f"üîç Scraping page {page}...")
                url = f"{self.base_url}/xdata?page={page}" if page > 1 else f"{self.base_url}/xdata"
                
                response = self.session.get(url)
                response.raise_for_status()
                soup = BeautifulSoup(response.content, 'html.parser')
                
                subscriptions, unsubscriptions = self.extract_page_data(soup)
                all_subscriptions.extend(subscriptions)
                all_unsubscriptions.extend(unsubscriptions)
                
                time.sleep(random.uniform(2, 4))
                
            except Exception as e:
                print(f"‚ö†Ô∏è Error scraping page {page}: {e}")
                continue
        
        return all_subscriptions, all_unsubscriptions
    
    def extract_page_data(self, soup):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –ø–æ–¥–ø–∏—Å–æ–∫ –∏ –æ—Ç–ø–∏—Å–æ–∫"""
        subscriptions = []
        unsubscriptions = []
        
        try:
            # –ü–æ–∏—Å–∫ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ —Å –¥–µ–π—Å—Ç–≤–∏—è–º–∏
            followed_elements = soup.find_all(text=re.compile(r'Followed', re.I))
            unfollowed_elements = soup.find_all(text=re.compile(r'Unfollowed', re.I))
            
            # –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø–æ–¥–ø–∏—Å–æ–∫
            for element in followed_elements:
                try:
                    container = element.parent.parent
                    links = container.find_all('a')
                    
                    if len(links) >= 2:
                        influencer_name = links[0].get_text(strip=True)
                        project_name = links[1].get_text(strip=True)
                        project_url = links[1].get('href')
                        
                        if project_url and not project_url.startswith('http'):
                            project_url = urljoin(self.base_url, project_url)
                        
                        subscriptions.append({
                            'influencer_name': influencer_name,
                            'project_name': project_name,
                            'project_url': project_url
                        })
                        
                except Exception:
                    continue
            
            # –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—Ç–ø–∏—Å–æ–∫
            for element in unfollowed_elements:
                try:
                    container = element.parent.parent
                    links = container.find_all('a')
                    
                    if len(links) >= 2:
                        influencer_name = links[0].get_text(strip=True)
                        
                        for link in links[1:]:
                            project_name = link.get_text(strip=True)
                            project_url = link.get('href')
                            
                            if project_url and not project_url.startswith('http'):
                                project_url = urljoin(self.base_url, project_url)
                            
                            unsubscriptions.append({
                                'influencer_name': influencer_name,
                                'project_name': project_name,
                                'project_url': project_url
                            })
                            
                except Exception:
                    continue
                    
        except Exception as e:
            print(f"‚ùå Error extracting data: {e}")
        
        return subscriptions, unsubscriptions
    
    def get_project_twitter(self, project_url):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ Twitter —Å—Å—ã–ª–∫–∏ –ø—Ä–æ–µ–∫—Ç–∞"""
        if not project_url:
            return None
            
        try:
            time.sleep(random.uniform(1, 2))
            
            response = self.session.get(project_url)
            response.raise_for_status()
            soup = BeautifulSoup(response.content, 'html.parser')
            
            for link in soup.find_all('a', href=True):
                href = link.get('href', '')
                if 'twitter.com' in href or 'x.com' in href:
                    if 'twitter.com' in href:
                        return href.split('?')[0]
                    elif 'x.com' in href:
                        return href.replace('x.com', 'twitter.com').split('?')[0]
            
            return None
            
        except Exception as e:
            print(f"‚ö†Ô∏è Error getting Twitter: {e}")
            return None
    
    def get_project_description(self, project_url):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –æ–ø–∏—Å–∞–Ω–∏—è –ø—Ä–æ–µ–∫—Ç–∞"""
        if not project_url:
            return "Crypto project"
            
        try:
            response = self.session.get(project_url)
            response.raise_for_status()
            soup = BeautifulSoup(response.content, 'html.parser')
            
            meta_desc = soup.find('meta', {'name': 'description'})
            if meta_desc:
                desc = meta_desc.get('content', '')
                if desc and len(desc) > 10:
                    return desc[:80] + '...' if len(desc) > 80 else desc
            
            return "Crypto project"
            
        except Exception:
            return "Crypto project"
    
    def enrich_project_data(self, entries, max_entries=5):
        """–û–±–æ–≥–∞—â–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –ø—Ä–æ–µ–∫—Ç–æ–≤"""
        enriched_entries = []
        
        for i, entry in enumerate(entries[:max_entries]):
            print(f"üîç Getting details for {entry['project_name']} ({i+1}/{min(len(entries), max_entries)})...")
            
            twitter_link = self.get_project_twitter(entry['project_url'])
            description = self.get_project_description(entry['project_url'])
            
            enriched_entry = {
                **entry,
                'twitter_link': twitter_link,
                'description': description
            }
            
            enriched_entries.append(enriched_entry)
            time.sleep(random.uniform(0.5, 1.5))
        
        return enriched_entries
    
    def format_message(self, subscriptions, unsubscriptions):
        """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–æ–æ–±—â–µ–Ω–∏—è"""
        date_str = datetime.now().strftime("%d.%m.%Y")
        
        message = f"üìÖ FollowTracker ‚Äî {date_str}\n"
        message += "üß† Daily Data:\n\n"
        
        # –ù–æ–≤—ã–µ –ø–æ–¥–ø–∏—Å–∫–∏
        message += "üìà New Subscriptions:\n"
        if subscriptions:
            for sub in subscriptions:
                influencer = sub['influencer_name']
                project_name = sub['project_name']
                description = sub.get('description', 'Crypto project')
                twitter_link = sub.get('twitter_link')
                
                if twitter_link:
                    project_text = f"[{project_name}]({twitter_link}) ({description})"
                else:
                    project_text = f"{project_name} ({description})"
                
                message += f"+ {influencer} ‚Üí {project_text}\n"
        else:
            message += "No new subscriptions detected today\n"
        
        # –û—Ç–ø–∏—Å–∫–∏
        message += "\nüìâ Unsubscriptions:\n"
        if unsubscriptions:
            for unsub in unsubscriptions:
                influencer = unsub['influencer_name']
                project_name = unsub['project_name']
                description = unsub.get('description', 'Crypto project')
                twitter_link = unsub.get('twitter_link')
                
                if twitter_link:
                    project_text = f"[{project_name}]({twitter_link}) ({description})"
                else:
                    project_text = f"{project_name} ({description})"
                
                message += f"- {influencer} ‚Üê {project_text}\n"
        else:
            message += "No unsubscriptions detected today\n"
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        message += f"\nüìä Daily Stats:\n"
        message += f"- Total new subscriptions: {len(subscriptions)}\n"
        message += f"- Total unsubscriptions: {len(unsubscriptions)}\n"
        
        if subscriptions:
            influencer_counts = {}
            for sub in subscriptions:
                name = sub['influencer_name']
                influencer_counts[name] = influencer_counts.get(name, 0) + 1
            
            most_active = max(influencer_counts.items(), key=lambda x: x[1])
            if most_active[1] > 1:
                message += f"- Most active influencer: {most_active[0]} ({most_active[1]} subscriptions)"
            else:
                message += f"- Most active influencer: Multiple influencers (1 subscription each)"
        else:
            message += f"- Most active influencer: N/A"
        
        return message
    
    async def send_message(self, message):
        """–û—Ç–ø—Ä–∞–≤–∫–∞ —Å–æ–æ–±—â–µ–Ω–∏—è –≤ Telegram"""
        try:
            await self.bot.send_message(
                chat_id=self.channel_id,
                text=message,
                parse_mode=ParseMode.MARKDOWN,
                disable_web_page_preview=True
            )
            print("‚úÖ Message sent successfully!")
            return True
        except Exception as e:
            print(f"‚ùå Error sending message: {e}")
            return False
    
    async def run(self):
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –∑–∞–ø—É—Å–∫–∞"""
        print("üöÄ Starting Crypto Follow Tracker...")
        
        # –°–∫—Ä–∞–ø–∏–Ω–≥ –¥–∞–Ω–Ω—ã—Ö
        subscriptions, unsubscriptions = self.scrape_xdata_pages()
        
        print(f"üìä Found {len(subscriptions)} subscriptions, {len(unsubscriptions
